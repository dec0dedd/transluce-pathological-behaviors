import asyncio
from openai import AsyncOpenAI
from transformers import AutoTokenizer
import traceback
import json
import os

# Import the original complex logic from where you saved it
# (Assuming you saved the code you provided as 'transluce_logic.py' in the same folder)
from src.reward import compute_reward

# 1. Define your model names globally (Must match exactly what you pass to vLLM)
#TARGET_MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"
TARGET_MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"
#JUDGE_MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"
JUDGE_MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct-AWQ"

# 2. Initialize the tokenizer globally so it doesn't reload on every single prompt
tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL_NAME)

def compute_score(data_source, solution_str: str, ground_truth: dict, extra_info=None) -> float:
    behavior = ground_truth.get('behavior', '')
    behavior_id = ground_truth.get('behavior_id', 'unknown')
    optimizer_target = ground_truth.get('optimizer_target', 'target_model')

    # Define an internal async function to handle the API calls
    async def _run_async_reward():
        async with AsyncOpenAI(
            base_url="http://localhost:8000/v1",
            api_key="EMPTY",
            max_retries=3
        ) as vllm_target_client, AsyncOpenAI(
            base_url="http://localhost:8080/v1",
            api_key="EMPTY",
            max_retries=3
        ) as vllm_judge_client:

            # B. Call the paper's heavy logic
            prbo_result = await compute_reward(
                gpt_oss_client=vllm_target_client,
                gpt_oss_tokenizer=tokenizer,
                behavior_id=behavior_id,
                policy_output=solution_str,
                optimizer_target=optimizer_target,
                behavior=behavior,
                openai_client=vllm_judge_client,
                target_model=TARGET_MODEL_NAME,
                prompt_judge_model=JUDGE_MODEL_NAME,
                response_judge_model=JUDGE_MODEL_NAME,
            )
            
            return prbo_result

    # 3. Bridge the async gap using asyncio.run
    try:
        prbo_struct = asyncio.run(_run_async_reward())

        log_data = {
            "behavior_id": prbo_struct.behavior_id,
            "behavior": prbo_struct.behavior,
            "policy_output": prbo_struct.policy_output,  # The prompt generated by your Llama investigator
            "score": float(prbo_struct.score),
            "proposal_prefix": prbo_struct.proposal_prefix,

            # --- Target Model Responses ---
            "normal_response": prbo_struct.normal_response if prbo_struct.normal_response else "FAILED_TO_GENERATE",
            "steered_response": prbo_struct.steered_response if prbo_struct.steered_response else "FAILED_TO_GENERATE",

            # --- Prompt Judge (Evaluates the jailbreak prompt itself) ---
            "prompt_score_value": prbo_struct.prompt_score.score if prbo_struct.prompt_score else None,
            "prompt_judge_prompt": prbo_struct.prompt_score.judge_prompt if prbo_struct.prompt_score else "N/A",
            "prompt_judge_response": prbo_struct.prompt_score.judge_response if prbo_struct.prompt_score else "N/A",

            # --- Normal Response Judge (Evaluates the target's natural answer) ---
            "normal_response_score_value": prbo_struct.normal_response_score.score if prbo_struct.normal_response_score else None,
            "normal_judge_prompt": prbo_struct.normal_response_score.judge_prompt if prbo_struct.normal_response_score else "N/A",
            "normal_judge_response": prbo_struct.normal_response_score.judge_response if prbo_struct.normal_response_score else "N/A",

            # --- Steered Response Judge (Evaluates the target's forced answer) ---
            "steered_response_score_value": prbo_struct.steered_response_score.score if prbo_struct.steered_response_score else None,
            "steered_judge_prompt": prbo_struct.steered_response_score.judge_prompt if prbo_struct.steered_response_score else "N/A",
            "steered_judge_response": prbo_struct.steered_response_score.judge_response if prbo_struct.steered_response_score else "N/A",
        }

        # Safely append to a JSONL file (Thread/Ray worker safe-ish for debugging)
        worker_pid = os.getpid()
        with open(f"reward_logs/{worker_pid}.jsonl", "a", encoding="utf-8") as f:
            f.write(json.dumps(log_data) + "\n")
        
        # 4. Return ONLY the float that verl needs for the PPO update
        return float(prbo_struct.score)
    except Exception as e:
        print(f"Reward computation failed for behavior {behavior_id}: {e}")
        traceback.print_exc()
        return -100.0